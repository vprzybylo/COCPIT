{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"6tGmiuOfY08czs2b4SHaHI2hw\",\n",
    "                        project_name=\"multi-campaigns\", workspace=\"vprzybylo\")\n",
    "experiment.log_code('/data/data/notebooks/Pytorch_mult_campaigns.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from pathlib import Path\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal pull from classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(train_imgs, nclasses):\n",
    "    #only weight the training dataset \n",
    "\n",
    "    class_sample_counts = [0] * nclasses\n",
    "    for item in train_imgs:  \n",
    "        class_sample_counts[item[1]] += 1\n",
    "    print('counts per class: ', class_sample_counts)\n",
    "\n",
    "#     weight_per_class = [0.] * nclasses\n",
    "#     N = float(sum(class_sample_counts))\n",
    "#     for i in range(nclasses): \n",
    "#         weight_per_class[i] = N/float(class_sample_counts[i])\n",
    "#     weight = [0] * len(images)\n",
    "#     for idx, val in enumerate(images):\n",
    "#         weight[idx] = weight_per_class[val[1]]\n",
    "\n",
    "    class_weights = 1./torch.Tensor(class_sample_counts)\n",
    "    train_targets = [sample[1] for sample in train_imgs]\n",
    "    train_samples_weights = [class_weights[class_id] for class_id in train_targets]\n",
    "\n",
    "    return class_sample_counts, torch.DoubleTensor(train_samples_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram_classcounts(class_names, class_counts):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    width = 0.75 # the width of the bars \n",
    "    ind = np.arange(len(class_counts))  # the x locations for the groups\n",
    "    ax.barh(class_names, class_counts, width, color=\"blue\", align='center', tick_label=class_names)\n",
    "    #ax.set_yticks(ind+width/2)\n",
    "    #plt.xticks(rotation=-90, ha='center')\n",
    "\n",
    "    for i, v in enumerate(class_counts):\n",
    "        ax.text(v, i-.1, str(v), color='blue')\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    #ax.set_xlim(0,2500)\n",
    "    plt.savefig('../plots/class_counts.png', dpi=300, format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_val(batch_size, train_data, val_data, class_names, show_sample=True, num_workers=32, valid_size=.8, k_fold=5):\n",
    "\n",
    "    #train_length = int(valid_size*len(all_data_wpath))\n",
    "    #val_length = len(all_data_wpath)-train_length\n",
    "    #train_data, val_data = torch.utils.data.random_split(all_data_wpath,(train_length,val_length))\n",
    "    #print(len(train_data), len(val_data))\n",
    "\n",
    "    # For an unbalanced dataset we create a weighted sampler\n",
    "    class_counts, train_samples_weights = make_weights_for_balanced_classes(train_data.dataset.imgs, len(range(num_classes)))\n",
    "    make_histogram_classcounts(class_names, class_counts)\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_samples_weights, \n",
    "                                                                   len(train_samples_weights),\n",
    "                                                                   replacement=True)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data.dataset, batch_size=batch_size,\n",
    "                                            sampler = train_sampler, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    val_sampler = SubsetRandomSampler(val_data.indices)\n",
    "    valloader = torch.utils.data.DataLoader(val_data.dataset, batch_size=batch_size,\n",
    "                                            sampler = val_sampler, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "#     val_samples_weights = make_weights_for_balanced_classes(val_data.dataset.imgs, len(range(num_classes)))\n",
    "#     val_sampler = torch.utils.data.sampler.WeightedRandomSampler(val_samples_weights, \n",
    "#                                                                    len(val_samples_weights),\n",
    "#                                                                    replacement=True)\n",
    "#     valloader = torch.utils.data.DataLoader(val_data.dataset, batch_size=batch_size,\n",
    "#                                             sampler = val_sampler, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    if show_sample:\n",
    "        show_sample(train_data, train_sampler)\n",
    "\n",
    "    torch.save(valloader, 'val_loader.pth')\n",
    "\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(train_data, train_sampler):\n",
    "\n",
    "    batch_size_sampler=20\n",
    "    sample_loader = torch.utils.data.DataLoader(train_data.dataset, batch_size=batch_size_sampler, \\\n",
    "                                                sampler = train_sampler, num_workers=1, drop_last=True)\n",
    "    data_iter = iter(sample_loader)\n",
    "\n",
    "    images, labels, paths = data_iter.next()\n",
    "    fig, ax = plt.subplots(batch_size_sampler//5, 5, figsize=(10, 8))\n",
    "\n",
    "    for j in range(images.size()[0]):\n",
    "\n",
    "        # Undo preprocessing\n",
    "        image = images[j].permute(1, 2, 0).cpu().numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "        image = std * image + mean\n",
    "\n",
    "        # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "        image = np.clip(image, 0, 1)\n",
    "        ax = ax.flatten()\n",
    "        ax[j].set_title(str(class_names[labels[j]]))\n",
    "        ax[j].axis('off')\n",
    "        ax[j].imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(datadir,\n",
    "                    batch_size,\n",
    "                    num_workers,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True):\n",
    "    \"\"\"\n",
    "    Utility function for loading and returning a multi-process\n",
    "    test iterator \n",
    "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - shuffle: whether to shuffle the dataset after every epoch.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - data_loader: test set iterator.\n",
    "    \"\"\"\n",
    "    transforms_ = transforms.Compose([transforms.Resize((224,224)),  #resizing helps memory usage\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    all_data_wpath = ImageFolderWithPaths(datadir,transform=transforms_)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(all_data_wpath,pin_memory=True,shuffle=shuffle,\n",
    "                    batch_size=batch_size, num_workers=num_workers)  \n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "def set_parameter_requires_grad(model, feature_extract):\n",
    "    if feature_extract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=False):\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"resnet152\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg16\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg19\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.squeezenet1_1(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(7,7), stride=(2,2))\n",
    "        #model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet169\":\n",
    "        #input_size = 224 \n",
    "        model_ft = models.densenet169(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"densenet201\":\n",
    "        #input_size = 224\n",
    "        model_ft = models.densenet201(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\"\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    elif model_name == \"efficient\":\n",
    "        #input_size=224\n",
    "        torch.hub.list('rwightman/gen-efficientnet-pytorch')\n",
    "        model_ft = torch.hub.load('rwightman/gen-efficientnet-pytorch', 'efficientnet_b0', pretrained=False)\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_logging(logger, loss, acc, step, model): \n",
    "\n",
    "    # 1. Log scalar values (scalar summary)\n",
    "    info = { 'loss': loss, 'accuracy': acc}\n",
    "\n",
    "    for tag, value in info.items():\n",
    "        logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "    for tag, value in model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "        logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n",
    "\n",
    "    # 3. Log training images (image summary)\n",
    "    #         denormalize = transforms.Normalize((-1,), (1 / 0.5,))\n",
    "    #         info = { 'images': demormalize(images)[:10].cpu().numpy() }\n",
    "\n",
    "    #         for tag, images in info.items():\n",
    "    #             logger.image_summary(tag, images, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, savename, dataloaders_dict, epochs, num_classes, experiment, is_inception, feature_extract=False):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #logger_train = Logger('./logs/'+current_time+'/train/')\n",
    "    #logger_val = Logger('./logs/'+current_time+'/val/')\n",
    "    model = initialize_model(model_name=model_name, num_classes=num_classes, feature_extract=feature_extract, use_pretrained=False)\n",
    "\n",
    "    def set_dropout(model, drop_rate=0.1):\n",
    "        for name, child in model.named_children():\n",
    "\n",
    "            if isinstance(child, torch.nn.Dropout):\n",
    "                child.p = drop_rate\n",
    "            set_dropout(child, drop_rate=drop_rate)\n",
    "    set_dropout(model, drop_rate=0.0)\n",
    "\n",
    "#     model.classifier = nn.Sequential(*[model.classifier()[i] for i in range(7) if i != 2 and i !=5])\n",
    "#     print(model.classifier())\n",
    "\n",
    "    #feature extract False for all layers to be updated\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.is_available())\n",
    "    # Send the model to GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                #print(\"\\t\",name)\n",
    "    #else:\n",
    "        #for name,param in model.named_parameters():\n",
    "            #if param.requires_grad == True:\n",
    "                #print(\"\\t\",name)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    # step_size: at how many multiples of epoch you decay\n",
    "    # step_size = 1, after every 1 epoch, new_lr = lr*gamma \n",
    "    # step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "    # gamma = decaying factor\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=0, verbose=True, eps=1e-04)\n",
    "    print(scheduler)\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss() #expects integer labels not one-hot encoded\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc_val = 0.0\n",
    "    since_total = time.time()\n",
    "\n",
    "    step = 0\n",
    "    label_counts = [0]*len(range(num_classes))\n",
    "    for epoch in range(epochs):\n",
    "        since_epoch = time.time()\n",
    "        #print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print('Phase: {}'.format(phase))\n",
    "            totals_train = 0\n",
    "            totals_val = 0\n",
    "            running_loss_train = 0.0\n",
    "            running_loss_val = 0.0\n",
    "            running_corrects_train = 0\n",
    "            running_corrects_val = 0\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "                #logger = logger_train\n",
    "\n",
    "            else:\n",
    "                model.eval()   \n",
    "                #logger = logger_val\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels, paths) in enumerate(dataloaders_dict[phase]):\n",
    "                for n in range(len(range(num_classes))):\n",
    "                    label_counts[n] += len(np.where(labels.numpy() == n)[0])\n",
    "\n",
    "#                 for n in range(len(range(num_classes))):\n",
    "#                     print(\"batch index {}, {} counts: {}\".format(\n",
    "#                         i, n, (labels == n).sum()))\n",
    "\n",
    "\n",
    "#                print('LABEL COUNT = ', label_counts)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                #print(inputs.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad() # a clean up step for PyTorch\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # makes sure to clear the intermediate values for evaluation\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # compute updates for each parameter\n",
    "\n",
    "                if phase == 'train':\n",
    "                    #Batch accuracy and loss statistics   \n",
    "                    batch_loss_train = loss.item() * inputs.size(0)     \n",
    "                    batch_corrects_train = torch.sum(preds == labels.data) \n",
    "                    #tensorboard_logging(logger, batch_loss_train, labels, batch_corrects_train, step, model)\n",
    "\n",
    "                    #for accuracy and loss statistics overall \n",
    "                    running_loss_train += loss.item() * inputs.size(0)\n",
    "                    running_corrects_train += torch.sum(preds == labels.data)\n",
    "                    totals_train += labels.size(0)\n",
    "\n",
    "                    if (i+1) % 5 == 0:\n",
    "                        print(\"Training, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1,\\\n",
    "                                                                      len(dataloaders_dict[phase]), \\\n",
    "                                                                      batch_loss_train/labels.size(0), \\\n",
    "                                                                      float(batch_corrects_train)/labels.size(0)))\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "                else:\n",
    "                    #Batch accuracy and loss statistics  \n",
    "                    batch_loss_val = loss.item() * inputs.size(0)     \n",
    "                    batch_corrects_val = torch.sum(preds == labels.data) \n",
    "\n",
    "                    #for accuracy and loss statistics overall\n",
    "                    running_loss_val += loss.item() * inputs.size(0)\n",
    "                    running_corrects_val += torch.sum(preds == labels.data)\n",
    "                    totals_val += labels.size(0)\n",
    "\n",
    "                    if (i+1) % 3 == 0:\n",
    "                        print(\"Validation, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1,\\\n",
    "                                                                      len(dataloaders_dict[phase]), \\\n",
    "                                                                      batch_loss_val/labels.size(0), \\\n",
    "                                                                      float(batch_corrects_val)/labels.size(0)))\n",
    "\n",
    "            if phase == 'train':\n",
    "                #epoch loss and accuracy stats\n",
    "                epoch_loss_train = running_loss_train / totals_train\n",
    "                epoch_acc_train = running_corrects_train.double() / totals_train\n",
    "                scheduler.step(epoch_acc_train) #reduce learning rate if not improving acc\n",
    "                experiment.log_metric('train scheduler', scheduler)\n",
    "\n",
    "                #with open('save_acc_loss_train_e50_bs128.csv', 'w', newline='') as file:\n",
    "                #    writer = csv.writer(file)\n",
    "                #    writer.writerow([model_name, epoch, epoch_acc_train, epoch_loss_train])\n",
    "\n",
    "                print(\"Training Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(epoch+1,epochs, epoch_loss_train, epoch_acc_train))\n",
    "                #tensorboard_logging(logger, epoch_loss_train, epoch_acc_train, epoch, model)\n",
    "                train_acc_history.append(epoch_acc_train)\n",
    "                train_loss_history.append(epoch_loss_train)\n",
    "                experiment.log_metric('epoch_acc_train', epoch_acc_train*100)\n",
    "                experiment.log_metric('epoch_loss_train', epoch_loss_train)\n",
    "\n",
    "            else: \n",
    "                epoch_loss_val = running_loss_val / totals_val\n",
    "                epoch_acc_val = running_corrects_val.double() / totals_val\n",
    "                scheduler.step(epoch_acc_val) #reduce learning rate if not improving acc\n",
    "                experiment.log_metric('val scheduler', scheduler)\n",
    "\n",
    "                #with open('save_acc_loss_val_e50_bs128.csv', 'w', newline='') as file:\n",
    "                #    writer = csv.writer(file)\n",
    "                #    writer.writerow([model_name, epoch, epoch_acc_val, epoch_loss_val])\n",
    "\n",
    "                print(\"Validation Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(epoch+1,epochs, epoch_loss_val, epoch_acc_val))\n",
    "                #tensorboard_logging(logger, epoch_loss_val, epoch_acc_val, epoch, model)\n",
    "                val_acc_history.append(epoch_acc_val)\n",
    "                val_loss_history.append(epoch_loss_val)\n",
    "                experiment.log_metric('epoch_acc_val', epoch_acc_val*100)\n",
    "                experiment.log_metric('epoch_loss_val', epoch_loss_val)\n",
    "\n",
    "                #deep copy the model\n",
    "                if epoch_acc_val > best_acc_val:\n",
    "                    #best_acc_val = epoch_acc_val\n",
    "                    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    # save/load best model weights\n",
    "                    if savename is not None:\n",
    "                        torch.save(model, savename+'_'+model_name)\n",
    "\n",
    "        time_elapsed = time.time() - since_epoch\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    time_elapsed = time.time() - since_total\n",
    "    print('All epochs comlete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    #with open('save_model_timing.csv', 'w', newline='') as file:\n",
    "        #writer = csv.writer(file)\n",
    "        #writer.writerow([model_name, time_elapsed])\n",
    "\n",
    "    return model, train_acc_history, val_acc_history, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #file = 'batch_size'\n",
    "    #writer = csv.writer(file)\n",
    "    for batch_size in params['batch_size']:\n",
    "        print('NEW BATCH SIZE: ', batch_size)\n",
    "    \n",
    "        all_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "        all_data_wpath = ImageFolderWithPaths(params['data_dir'],transform=all_transforms) #custom dataset that includes entire path\n",
    "\n",
    "        train_score = pd.Series()\n",
    "        val_score = pd.Series()\n",
    "\n",
    "        total_size = len(all_data_wpath)\n",
    "        fraction = 1/params['kfold']\n",
    "        seg = int(total_size * fraction)\n",
    "        # tr:train,val:valid; r:right,l:left;  eg: trrr: right index of right side train subset \n",
    "        # index: [trll,trlr],[vall,valr],[trrl,trrr]\n",
    "        \n",
    "        model_train_accs = []\n",
    "        model_val_accs = []\n",
    "        model_train_loss = []\n",
    "        model_val_loss = []\n",
    "        for i in range(params['kfold']):\n",
    "            trll = 0\n",
    "            trlr = i * seg\n",
    "            vall = trlr\n",
    "            valr = i * seg + seg\n",
    "            trrl = valr\n",
    "            trrr = total_size\n",
    "\n",
    "            print(\"train indices: [%d,%d),[%d,%d), test indices: [%d,%d)\" \n",
    "              % (trll,trlr,trrl,trrr,vall,valr))\n",
    "\n",
    "            train_left_indices = list(range(trll,trlr))\n",
    "            train_right_indices = list(range(trrl,trrr))\n",
    "\n",
    "            train_indices = train_left_indices + train_right_indices\n",
    "            val_indices = list(range(vall,valr))\n",
    "            \n",
    "            train_data = torch.utils.data.dataset.Subset(all_data_wpath,train_indices)\n",
    "            val_data = torch.utils.data.dataset.Subset(all_data_wpath,val_indices)\n",
    "\n",
    "            train_loader, val_loader = load_split_train_val(\n",
    "                batch_size,\n",
    "                train_data,\n",
    "                val_data,\n",
    "                class_names=params['class_names'], \n",
    "                show_sample=False,\n",
    "                num_workers=num_workers)\n",
    "\n",
    "            dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "\n",
    "            for model_name in params['model_names']: \n",
    "                for epochs in params['max_epochs']:\n",
    "                    model_ft, train_acc_history, val_acc_history, train_loss_history, val_loss_history= train_model(\n",
    "                        model_name,\n",
    "                        params['savename'],\n",
    "                        dataloaders_dict,\n",
    "                        epochs,\n",
    "                        num_classes,\n",
    "                        experiment,\n",
    "                        is_inception=False)\n",
    "\n",
    "                    model_val_accs.append(val_acc_history)\n",
    "                    model_train_accs.append(train_acc_history)\n",
    "                    model_train_loss.append(train_loss_history)\n",
    "                    model_val_loss.append(val_loss_history)\n",
    "\n",
    "    return model_name, model_train_accs, model_val_accs, model_train_loss, model_val_loss, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    params = {'kfold':5,\n",
    "            'lr': [0.01],\n",
    "            'batch_size': [128],\n",
    "            'max_epochs': [50],\n",
    "            'data_dir':'../cpi_data/training_datasets/hand_labeled_resized_multcampaigns_clean/',\n",
    "            #'momentum': [0.9, 0.999], \n",
    "            'class_names':['aggregates','blank','blurry','budding','bullets','columns','compact irregulars',\\\n",
    "                           'fragments','needles','plates','rimed aggregates','rimed columns','spheres'],\n",
    "            'model_names':['efficient'],\n",
    "            #'model_names':['resnet18', 'resnet34', 'resnet152', 'alexnet', 'vgg16', 'vgg19', 'densenet169', 'densenet201'],\n",
    "            'savename': '../saved_models/bs128_e50_13classes_clean_efficient'}\n",
    "            #'savename': None}\n",
    "\n",
    "    experiment.log_parameters(params)\n",
    "    #experiment.add_tag('inlcudes bad data')\n",
    "    num_workers = 20  #change to # of cores available to load images\n",
    "    num_classes = len(params['class_names'])\n",
    "    #experiment.display()\n",
    "    model_name, model_train_accs, model_val_accs, model_train_loss, model_val_loss, train_loader, val_loader = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY PLOT for training and validation\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i.cpu().numpy()*100 for i in model_train_accs[0]], label='train')\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i.cpu().numpy()*100 for i in model_val_accs[0]], label='validation')\n",
    " \n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params['max_epochs'][0]+1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "#LOSS PLOT for training and validation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i for i in model_train_loss[0]], label='train')\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i for i in model_val_loss[0]], label='validation')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params['max_epochs'][0]+1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Confusion Matrix - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../saved_models/vgg19_bs128_e20_13classes_clean').cuda()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Send the model to GPU\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "all_preds= []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (imgs, labels, img_paths) in enumerate(val_loader):\n",
    "        # get the inputs\n",
    "        inputs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "        pred = torch.argmax(output, 1)\n",
    "\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZED\n",
    "\n",
    "cm = confusion_matrix(np.asarray(list(itertools.chain(*all_preds))), np.asarray(list(itertools.chain(*all_labels))))\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(13,9))\n",
    "\n",
    "heat = sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=params['class_names'], yticklabels=params['class_names'], cmap=\"Blues\")\n",
    "heat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=18)\n",
    "heat.set_yticklabels(heat.get_xticklabels(), rotation=0, fontsize=18)\n",
    "\n",
    "\n",
    "plt.ylabel('Actual Labels', fontsize=20)\n",
    "plt.xlabel('Predicted Labels', fontsize=20);\n",
    "plt.savefig('../plots/norm_conf_matrix.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.asarray(list(itertools.chain(*all_preds))), np.asarray(list(itertools.chain(*all_labels))))\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "heat = sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=params['class_names'], yticklabels=params['class_names'], cmap=\"Blues\")\n",
    "heat.set_xticklabels(heat.get_xticklabels(), rotation=90)\n",
    "plt.ylabel('Actual Labels', fontsize=20)\n",
    "plt.xlabel('Predicted Labels', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics classification report\n",
    "classification_report(all_labels, all_preds, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning method\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 50\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue','darkblue','gold','red', 'darkred', 'lightgreen', 'darkgreen']\n",
    "for i, (model, train_accs, val_accs) in enumerate(zip(params['model_names'], model_train_accs, model_val_accs)):\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in train_accs[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in val_accs[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "plt.ylim(20,100)\n",
    "plt.xlim(1,num_epochs)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for i, (model,train_loss, val_loss) in enumerate(zip(params['model_names'], model_train_loss, model_val_loss)):\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in train_loss[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in val_loss[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.ylim(0,2.4)\n",
    "plt.xlim(1,num_epochs)\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2))\n",
    "plt.tight_layout()\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "# fig.savefig('cpi_data/OLYMPEX/plots/loss_acc_allmodels_reducelr_all_512_0dp.eps')\n",
    "# fig.savefig('cpi_data/OLYMPEX/plots/loss_acc_allmodels_reducelr_all_512_0dp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning method\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 50\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue','darkblue','gold','red', 'darkred', 'lightgreen', 'darkgreen']\n",
    "for i, (model, train_accs, val_accs) in enumerate(zip(params['model_names'], model_train_accs, model_val_accs)):\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in train_accs[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in val_accs[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "plt.ylim(40,100)\n",
    "plt.xlim(1,num_epochs)\n",
    "ax1.legend(title='Model type:', loc='lower right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for i, (model,train_loss, val_loss) in enumerate(zip(params['model_names'], model_train_loss, model_val_loss)):\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in train_loss[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in val_loss[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "ax2.legend(title='Model type:', loc='upper right', prop={'size': 10})\n",
    "plt.ylim(0,2.4)\n",
    "plt.xlim(1,num_epochs)\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2))\n",
    "plt.tight_layout()\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "#fig.savefig('../plots/loss_acc_allmodels_bs_128_e20_13classes.eps', dpi=300)\n",
    "#fig.savefig('../plots/loss_acc_allmodels_bs_128_e20_13classes.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,5))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "num_epochs = 20\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "colors = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "color_key = {'vgg19': 'darkred', 'vgg16': 'red', 'resnet34':'salmon', 'resnet18':'lightsalmon', 'resnet152':'bisque',\n",
    "            'densenet201':'lightgreen', 'densenet169':'darkgreen', 'alexnet':'lightblue'}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = [i.cpu().numpy()*100 for i in np.array(model_train_accs)[:,epoch]]\n",
    "    model_names, train_acc, colors_sorted = (list(x) for x in zip(*sorted(zip(params['model_names'], train_acc, colors), reverse=True)))\n",
    "    colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax1.bar(epoch+1, train_acc[m], color=colors_sorted[m], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, train_acc, color=colors_sorted)\n",
    "        \n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = [i for i in np.array(model_train_loss)[:,epoch]]\n",
    "    train_loss, model_names = (list(x) for x in zip(*sorted(zip(train_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#plt.ylim(0,2)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "\n",
    "#colors = plt.cm.rainbow(np.linspace(0,1,9))\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    model_names, val_acc, colors_sorted = (list(x) for x in zip(*sorted(zip(params['model_names'], val_acc, colors), reverse=True)))\n",
    "\n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name])\n",
    "        \n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax3.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax3.yaxis.set_ticks_position('both')\n",
    "ax3.minorticks_on()\n",
    "ax3.tick_params(axis='y', which='minor', direction='out')\n",
    "ax3.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_loss, params['model_names'], colors), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "plt.ylim(0,2)\n",
    "ax4.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax4.yaxis.set_ticks_position('both')\n",
    "ax4.minorticks_on()\n",
    "ax4.tick_params(axis='y', which='minor', direction='out')\n",
    "ax4.xaxis.set_tick_params(which='minor', bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,5))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "num_epochs = 20\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "colors = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "color_key = {'vgg19': 'darkred', 'vgg16': 'red', 'resnet34':'salmon', 'resnet18':'lightsalmon', 'resnet152':'bisque',\n",
    "            'densenet201':'lightgreen', 'densenet169':'darkgreen', 'alexnet':'lightblue'}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = [i.cpu().numpy()*100 for i in np.array(model_train_accs)[:,epoch]]\n",
    "    train_acc, model_names = (list(x) for x in zip(*sorted(zip(train_acc, params['model_names']), reverse=True)))\n",
    "    #colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']     \n",
    "            \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            print(color_key[model_name], train_acc[m], model_name)\n",
    "            p1 = ax1.bar(epoch+1, train_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, train_acc, color=colors_sorted)\n",
    "        \n",
    "plt.ylim(30,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = [i for i in np.array(model_train_loss)[:,epoch]]\n",
    "    train_loss, model_names = (list(x) for x in zip(*sorted(zip(train_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#plt.ylim(0,2)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "colors_sorted = ['darkred', 'red', 'salmon', \\\n",
    "                 'lightsalmon','bisque', 'lightgreen', \\\n",
    "                 'darkgreen', 'lightblue']  \n",
    "\n",
    "#colors = plt.cm.rainbow(np.linspace(0,1,9))\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    val_acc, model_names = (list(x) for x in zip(*sorted(zip(val_acc, params['model_names']), reverse=True)))\n",
    "\n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            print(color_key[model_name], val_acc[m], model_name)\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name])\n",
    "        \n",
    "plt.ylim(30,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax3.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax3.yaxis.set_ticks_position('both')\n",
    "ax3.minorticks_on()\n",
    "ax3.tick_params(axis='y', which='minor', direction='out')\n",
    "ax3.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names= (list(x) for x in zip(*sorted(zip(val_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "plt.ylim(0,2)\n",
    "ax4.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax4.yaxis.set_ticks_position('both')\n",
    "ax4.minorticks_on()\n",
    "ax4.tick_params(axis='y', which='minor', direction='out')\n",
    "ax4.xaxis.set_tick_params(which='minor', bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = np.array(model_train_accs).transpose(1,0)*100\n",
    "df_train = pd.DataFrame(train_accs, index=epochs, columns=params['model_names'], dtype = np.float64)\n",
    "val_accs = np.array(model_val_accs).transpose(1,0)*100\n",
    "df_val = pd.DataFrame(val_accs, index=epochs, columns=params['model_names'], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(kind='bar', colormap='rainbow', stacked=False, figsize=(12,5), ylim=[20,120], xlim=[0,29]).legend(\n",
    "    loc='upper center', ncol=4, title=\"Model Name\")\n",
    "plt.axhline(y=100, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "\n",
    "df_val.plot(kind='bar', colormap='rainbow', stacked=False, figsize=(12,5), ylim=[20,120], xlim=[0,29]).legend(\n",
    "    loc='upper center', ncol=4, title=\"Model Name\")\n",
    "plt.axhline(y=100, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 10\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue', 'darkblue', 'gold','red', 'darkred', 'lightgreen', 'darkgreen']  \n",
    "width = 0.35\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    val_acc, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_acc, params['model_names'], colors), reverse=True)))\n",
    "\n",
    "    if epoch == 0: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax1.bar(epoch+1, val_acc[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "            p1 = ax1.bar(epoch+1+width, val_acc[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, val_acc, color=colors_sorted)\n",
    "\n",
    "plt.ylim(20,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_loss, params['model_names'], colors), reverse=True)))\n",
    "\n",
    "    if epoch == 0: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, val_loss[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "    else:\n",
    "        p1 = ax2.bar(epoch+1, val_loss, color=colors_sorted)\n",
    "\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Predictions on Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns a Numpy array\n",
    "    '''\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(path, model, topk=9):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    img = Image.open(path)\n",
    "    img = img.convert('RGB')\n",
    "    img = process_image(img)\n",
    "    \n",
    "    # Convert 2D image to 1D vector\n",
    "    img = np.expand_dims(img, 0)\n",
    "\n",
    "    img = torch.from_numpy(img)\n",
    "    \n",
    "    model.eval()\n",
    "    inputs = Variable(img).to(device)\n",
    "    logits = model.forward(inputs)\n",
    "    \n",
    "    ps = F.softmax(logits,dim=1)\n",
    "    topk = ps.cpu().topk(topk)\n",
    "    \n",
    "    return (e.data.numpy().squeeze().tolist() for e in topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(im, prob, crystal_names):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    \n",
    "    image = Image.open(im)\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(7, 10), ncols=1, nrows=2)\n",
    "    \n",
    "    ax1.set_title(crystal_names[0])\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    y_pos = np.arange(len(prob))\n",
    "    ax2.barh(y_pos, prob, align='center')\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(crystal_names)\n",
    "    ax2.tick_params(axis='y', rotation=45)\n",
    "    ax2.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax2.set_title('Class Probability')\n",
    "    plt.show()\n",
    "    #current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #fig.savefig('classify/'+current_time+'.png',bbox_inches='tight',pad_inches=.3)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load('../saved_models/vgg19_bs128_e20_13classes').cuda()\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_loader = torch.load('../saved_models/val_loader.pth')\n",
    "for batch_idx, (imgs, labels, img_paths) in enumerate(val_loader):\n",
    "    #predictions = model_ft(imgs)\n",
    "    #preds = torch.max(predictions, 1).indices.tolist()    \n",
    "    \n",
    "    for im in img_paths:\n",
    "        probs, classes = predict2(im, model.to(device))  \n",
    "        crystal_names = [params['class_names'][e] for e in classes]\n",
    "        view_classify(im, probs, crystal_names)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(data_dir, save_dir, crystal_names, im):\n",
    "    image = Image.open(data_dir + im).convert(\"RGB\")\n",
    "    if crystal_names[0] == 'rimed aggs':\n",
    "        crystal_names[0] = 'rimed_aggs'\n",
    "    if crystal_names[0] == 'rimed columns':\n",
    "        crystal_names[0] = 'rimed_columns'\n",
    "    if crystal_names[0] == 'compact irregulars':\n",
    "         crystal_names[0] = 'compact_irregulars'\n",
    "    if not os.path.exists(save_dir+crystal_names[0]):\n",
    "        os.makedirs(save_dir+crystal_names[0])\n",
    "    image.save(save_dir+crystal_names[0]+'/'+im) \n",
    "    # cv2.imwrite(save_dir+crystal_names[0]+'/'+im, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on new data - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc)\n",
    "        #print(image)\n",
    "        #image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        tensor_image = self.transform(image)\n",
    "        path = self.total_imgs[idx]\n",
    "        return tensor_image, path\n",
    "\n",
    "model = torch.load('../saved_models/bs128_e50_13classes_clean_vgg19').cuda()\n",
    "model.eval()\n",
    "campaign = 'ARM'\n",
    "data_dir = '../cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "#save_dir = 'cpi_data/campaigns/'+campaign+'/'\n",
    "\n",
    "#apply same transforms\n",
    "test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testdata = TestDataSet(data_dir, transform=test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=100, shuffle=False, \n",
    "                               num_workers=20, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, open_dir, file_list):\n",
    "        self.desired_size = 1000\n",
    "        self.open_dir = open_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        self.all_paths = natsorted(file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.open_dir, self.all_paths[idx])\n",
    "        #image = Image.open(img_path)\n",
    "        \n",
    "        #training images were resized to 1000x1000 initially\n",
    "        image = cv2.cvtColor(cv2.imread(self.open_dir+self.all_paths[idx], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.desired_size, self.desired_size), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        image = Image.fromarray(image) #convert back to PIL for transforms\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        path = self.all_paths[idx]\n",
    "        return (image, path)\n",
    "    \n",
    "model = torch.load('../saved_models/bs128_e50_13classes_clean_vgg19').cuda()\n",
    "model.eval()\n",
    "campaign = 'MPACE'\n",
    "df = pd.read_pickle('../final_databases/no_mask/df_good_ice_'+campaign+'.pkl')\n",
    "data_dir = '../cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "testdata = TestDataSet(data_dir, df['filename'])\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=100, shuffle=False, \n",
    "                               num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for batch_idx, (imgs, img_paths) in enumerate(test_loader):\n",
    "    #predictions = model_ft(imgs)\n",
    "    #preds = torch.max(predictions, 1).indices.tolist()   \n",
    "    for im in img_paths:\n",
    "        path = data_dir + im\n",
    "        probs, classes = predict2(path, model.to(device))\n",
    "        crystal_names = [params['class_names'][e] for e in classes]\n",
    "        view_classify(path, probs, crystal_names)\n",
    "        #save_image(data_dir, save_dir, crystal_names, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_idx, (imgs, img_paths) in enumerate(test_loader):\n",
    "    for im in img_paths:\n",
    "        path = data_dir+im\n",
    "        img_og = Image.open(path)\n",
    "        img = img_og.convert('RGB')\n",
    "        img = process_image(img)\n",
    "\n",
    "        # Convert 2D image to 1D vector\n",
    "        img = np.expand_dims(img, 0)\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "        prediction = model(img)\n",
    "        cpu_pred = prediction.cpu()\n",
    "        result = cpu_pred.data.numpy()\n",
    "        print(class_names[result.argmax()])\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.imshow(img_og)\n",
    "        plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
