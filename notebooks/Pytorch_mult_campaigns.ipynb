{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/vprzybylo/multi-campaigns/ee93461103f3482892b60d1167e9e06c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "experiment = Experiment(api_key=\"6tGmiuOfY08czs2b4SHaHI2hw\",\n",
    "                        project_name=\"multi-campaigns\", workspace=\"vprzybylo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from pathlib import Path\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_params = {'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'xx-large'}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal pull from classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(train_imgs, nclasses):   \n",
    "    #only weight the training dataset \n",
    "    \n",
    "    class_sample_counts = [0] * nclasses                                                      \n",
    "    for item in train_imgs:  \n",
    "        class_sample_counts[item[1]] += 1      \n",
    "    print('counts per class: ', class_sample_counts)\n",
    "    \n",
    "#     weight_per_class = [0.] * nclasses                                      \n",
    "#     N = float(sum(class_sample_counts))                                                   \n",
    "#     for i in range(nclasses): \n",
    "#         weight_per_class[i] = N/float(class_sample_counts[i])                                 \n",
    "#     weight = [0] * len(images)                                              \n",
    "#     for idx, val in enumerate(images):    \n",
    "#         weight[idx] = weight_per_class[val[1]]  \n",
    "        \n",
    "    class_weights = 1./torch.Tensor(class_sample_counts)\n",
    "    train_targets = [sample[1] for sample in train_imgs]\n",
    "    train_samples_weights = [class_weights[class_id] for class_id in train_targets]\n",
    "\n",
    "    return class_sample_counts, torch.DoubleTensor(train_samples_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram_classcounts(class_names, class_counts):\n",
    "    fig, ax = plt.subplots(figsize=(8,5))    \n",
    "\n",
    "    width = 0.75 # the width of the bars \n",
    "    ind = np.arange(len(class_counts))  # the x locations for the groups\n",
    "    ax.barh(class_names, class_counts, width, color=\"blue\", align='center', tick_label=class_names)\n",
    "    #ax.set_yticks(ind+width/2)\n",
    "    #plt.xticks(rotation=-90, ha='center')\n",
    "    \n",
    "    for i, v in enumerate(class_counts):\n",
    "        ax.text(v, i-.1, str(v), color='blue')\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    #ax.set_xlim(0,2500)\n",
    "    plt.savefig('../plots/class_counts.png', dpi=300, format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_val(class_names, datadir, batch_size, show_sample=True, num_workers=32, valid_size = .8):\n",
    "    \n",
    "    all_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    all_data_wpath = ImageFolderWithPaths(datadir,transform=all_transforms) #custom dataset that includes entire path\n",
    "    \n",
    "#     num_train = len(all_data_wpath)\n",
    "#     indices = list(range(num_train))\n",
    "#     split = int(np.floor(valid_size * num_train))\n",
    "#     np.random.shuffle(indices)\n",
    "#     train_idx, val_idx = indices[split:], indices[:split-1]\n",
    "    \n",
    "#     train_data = torch.utils.data.Subset(all_data_wpath, train_idx)\n",
    "#     val_data = torch.utils.data.Subset(all_data_wpath, val_idx)\n",
    "    \n",
    "    train_length = int(valid_size*len(all_data_wpath))\n",
    "    val_length = len(all_data_wpath)-train_length\n",
    "    train_data, val_data = torch.utils.data.random_split(all_data_wpath,(train_length,val_length))\n",
    "    #print(len(train_data), len(val_data))\n",
    "    \n",
    "    # For an unbalanced dataset we create a weighted sampler              \n",
    "    class_counts, train_samples_weights = make_weights_for_balanced_classes(train_data.dataset.imgs, len(range(num_classes)))                                                                   \n",
    "    make_histogram_classcounts(class_names, class_counts)\n",
    "    \n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_samples_weights, \n",
    "                                                                   len(train_samples_weights),\n",
    "                                                                   replacement=True)                     \n",
    "    trainloader = torch.utils.data.DataLoader(train_data.dataset, batch_size=batch_size,                         \n",
    "                                            sampler = train_sampler, num_workers=num_workers, pin_memory=True)    \n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(val_data.indices)                 \n",
    "    valloader = torch.utils.data.DataLoader(val_data.dataset, batch_size=batch_size,                             \n",
    "                                            sampler = val_sampler, num_workers=num_workers, pin_memory=True)  \n",
    "\n",
    "#     val_samples_weights = make_weights_for_balanced_classes(val_data.dataset.imgs, len(range(num_classes)))                                                                   \n",
    "    \n",
    "#     val_sampler = torch.utils.data.sampler.WeightedRandomSampler(val_samples_weights, \n",
    "#                                                                    len(val_samples_weights),\n",
    "#                                                                    replacement=True)                     \n",
    "#     valloader = torch.utils.data.DataLoader(val_data.dataset, batch_size=batch_size,                              \n",
    "#                                             sampler = val_sampler, num_workers=num_workers, pin_memory=True)    \n",
    "    \n",
    "    if show_sample:\n",
    "        show_sample(train_data, train_sampler)\n",
    "        \n",
    "    torch.save(valloader, 'val_loader.pth')\n",
    "            \n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(train_data, train_sampler):\n",
    "    \n",
    "    batch_size_sampler=20\n",
    "    sample_loader = torch.utils.data.DataLoader(train_data.dataset, batch_size=batch_size_sampler, \\\n",
    "                                                sampler = train_sampler, num_workers=1, drop_last=True)\n",
    "\n",
    "    data_iter = iter(sample_loader)\n",
    "\n",
    "    images, labels, paths = data_iter.next()\n",
    "    fig, ax = plt.subplots(batch_size_sampler//5, 5, figsize=(10, 8))\n",
    "\n",
    "    for j in range(images.size()[0]):\n",
    "        \n",
    "        # Undo preprocessing\n",
    "        image = images[j].permute(1, 2, 0).cpu().numpy()\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "        image = std * image + mean\n",
    "\n",
    "        # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "        image = np.clip(image, 0, 1)\n",
    "        ax = ax.flatten()\n",
    "        ax[j].set_title(str(class_names[labels[j]]))\n",
    "        ax[j].axis('off') \n",
    "        ax[j].imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(datadir,\n",
    "                    batch_size,\n",
    "                    num_workers,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True):\n",
    "    \"\"\"\n",
    "    Utility function for loading and returning a multi-process\n",
    "    test iterator \n",
    "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - shuffle: whether to shuffle the dataset after every epoch.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - data_loader: test set iterator.\n",
    "    \"\"\"\n",
    "    transforms_ = transforms.Compose([transforms.Resize(224),  #resizing helps memory usage\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    all_data_wpath = ImageFolderWithPaths(datadir,transform=transforms_)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(all_data_wpath,pin_memory=True,shuffle=shuffle,\n",
    "                    batch_size=batch_size, num_workers=num_workers)  \n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "def set_parameter_requires_grad(model, feature_extract):\n",
    "    if feature_extract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=False):\n",
    "\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    # variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"resnet34\":\n",
    "        model_ft = models.resnet34(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"resnet152\":\n",
    "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg16\":\n",
    "        \"\"\" VGG\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg19\":\n",
    "        \"\"\" VGG\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_1(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(7,7), stride=(2,2))\n",
    "        #model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet169\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\" \n",
    "        model_ft = models.densenet169(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"densenet201\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\" \n",
    "        model_ft = models.densenet201(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_logging(logger, loss, acc, step, model): \n",
    "    \n",
    "    # 1. Log scalar values (scalar summary)\n",
    "    info = { 'loss': loss, 'accuracy': acc}\n",
    "\n",
    "    for tag, value in info.items():\n",
    "        logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "    for tag, value in model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "        logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n",
    "\n",
    "    # 3. Log training images (image summary)\n",
    "    #         denormalize = transforms.Normalize((-1,), (1 / 0.5,))\n",
    "    #         info = { 'images': demormalize(images)[:10].cpu().numpy() }\n",
    "\n",
    "    #         for tag, images in info.items():\n",
    "    #             logger.image_summary(tag, images, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, savename, dataloaders_dict, epochs, num_classes, experiment, is_inception, feature_extract=False):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #logger_train = Logger('./logs/'+current_time+'/train/')\n",
    "    #logger_val = Logger('./logs/'+current_time+'/val/')\n",
    "    model, input_size = initialize_model(model_name=model_name, num_classes=num_classes, feature_extract=feature_extract, use_pretrained=False)\n",
    "    \n",
    "    def set_dropout(model, drop_rate=0.1):\n",
    "        for name, child in model.named_children():\n",
    "            \n",
    "            if isinstance(child, torch.nn.Dropout):\n",
    "                child.p = drop_rate\n",
    "            set_dropout(child, drop_rate=drop_rate)\n",
    "    set_dropout(model, drop_rate=0.0)\n",
    "    print(model)\n",
    "    \n",
    "#     model.classifier = nn.Sequential(*[model.classifier()[i] for i in range(7) if i != 2 and i !=5])\n",
    "#     print(model.classifier())\n",
    "\n",
    "    #feature extract False for all layers to be updated\n",
    "   \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(torch.cuda.is_available())\n",
    "    # Send the model to GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        \n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                #print(\"\\t\",name)\n",
    "    #else:\n",
    "        #for name,param in model.named_parameters():\n",
    "            #if param.requires_grad == True:\n",
    "                #print(\"\\t\",name)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    # step_size: at how many multiples of epoch you decay\n",
    "    # step_size = 1, after every 1 epoch, new_lr = lr*gamma \n",
    "    # step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "    # gamma = decaying factor\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=0, verbose=True, eps=1e-04)\n",
    "    print(scheduler)\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss() #expects integer labels not one-hot encoded\n",
    "     \n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc_val = 0.0\n",
    "    since_total = time.time()\n",
    "    \n",
    "    step = 0\n",
    "    label_counts = [0]*len(range(num_classes))\n",
    "    for epoch in range(epochs):\n",
    "        since_epoch = time.time()\n",
    "        #print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-' * 20)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print('Phase: {}'.format(phase))\n",
    "            totals_train = 0\n",
    "            totals_val = 0\n",
    "            running_loss_train = 0.0\n",
    "            running_loss_val = 0.0\n",
    "            running_corrects_train = 0\n",
    "            running_corrects_val = 0\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "                #logger = logger_train\n",
    "                \n",
    "            else:\n",
    "                model.eval()   \n",
    "                #logger = logger_val\n",
    "            \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels, paths) in enumerate(dataloaders_dict[phase]):\n",
    "                for n in range(len(range(num_classes))):\n",
    "                    label_counts[n] += len(np.where(labels.numpy() == n)[0])\n",
    "                    \n",
    "#                 for n in range(len(range(num_classes))):\n",
    "#                     print(\"batch index {}, {} counts: {}\".format(\n",
    "#                         i, n, (labels == n).sum()))\n",
    "\n",
    "                \n",
    "#                print('LABEL COUNT = ', label_counts)\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                #print(inputs.device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad() # a clean up step for PyTorch\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # makes sure to clear the intermediate values for evaluation\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # compute updates for each parameter\n",
    "                        optimizer.step() # make the updates for each parameter                        \n",
    "\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    #Batch accuracy and loss statistics   \n",
    "                    batch_loss_train = loss.item() * inputs.size(0)     \n",
    "                    batch_corrects_train = torch.sum(preds == labels.data) \n",
    "                    #tensorboard_logging(logger, batch_loss_train, labels, batch_corrects_train, step, model)\n",
    "                    \n",
    "                    #for accuracy and loss statistics overall \n",
    "                    running_loss_train += loss.item() * inputs.size(0)\n",
    "                    running_corrects_train += torch.sum(preds == labels.data)\n",
    "                    totals_train += labels.size(0)\n",
    "                    \n",
    "                    if (i+1) % 5 == 0:\n",
    "                        print(\"Training, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1,\\\n",
    "                                                                      len(dataloaders_dict[phase]), \\\n",
    "                                                                      batch_loss_train/labels.size(0), \\\n",
    "                                                                      float(batch_corrects_train)/labels.size(0)))\n",
    "\n",
    "                    step += 1\n",
    "                    \n",
    "                else:\n",
    "                    #Batch accuracy and loss statistics  \n",
    "                    batch_loss_val = loss.item() * inputs.size(0)     \n",
    "                    batch_corrects_val = torch.sum(preds == labels.data) \n",
    "                    \n",
    "                    \n",
    "                    #for accuracy and loss statistics overall\n",
    "                    running_loss_val += loss.item() * inputs.size(0)\n",
    "                    running_corrects_val += torch.sum(preds == labels.data)\n",
    "                    totals_val += labels.size(0)\n",
    "                    \n",
    "                    if (i+1) % 3 == 0:\n",
    "                        print(\"Validation, Batch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(i+1,\\\n",
    "                                                                      len(dataloaders_dict[phase]), \\\n",
    "                                                                      batch_loss_val/labels.size(0), \\\n",
    "                                                                      float(batch_corrects_val)/labels.size(0)))\n",
    "\n",
    "            if phase == 'train':\n",
    "                #epoch loss and accuracy stats    \n",
    "                epoch_loss_train = running_loss_train / totals_train\n",
    "                epoch_acc_train = running_corrects_train.double() / totals_train\n",
    "                scheduler.step(epoch_acc_train) #reduce learning rate if not improving acc\n",
    "                experiment.log_metric('train scheduler', scheduler)\n",
    "\n",
    "                #with open('save_acc_loss_train_e50_bs128.csv', 'w', newline='') as file:\n",
    "                #    writer = csv.writer(file)\n",
    "                #    writer.writerow([model_name, epoch, epoch_acc_train, epoch_loss_train])\n",
    "\n",
    "                print(\"Training Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(epoch+1,epochs, epoch_loss_train, epoch_acc_train))\n",
    "                #tensorboard_logging(logger, epoch_loss_train, epoch_acc_train, epoch, model)\n",
    "                train_acc_history.append(epoch_acc_train)\n",
    "                train_loss_history.append(epoch_loss_train)\n",
    "                experiment.log_metric('epoch_acc_train', epoch_acc_train*100)\n",
    "                experiment.log_metric('epoch_loss_train', epoch_loss_train)\n",
    "\n",
    "            else: \n",
    "                epoch_loss_val = running_loss_val / totals_val\n",
    "                epoch_acc_val = running_corrects_val.double() / totals_val\n",
    "                scheduler.step(epoch_acc_val) #reduce learning rate if not improving acc\n",
    "                experiment.log_metric('val scheduler', scheduler)\n",
    "                \n",
    "                #with open('save_acc_loss_val_e50_bs128.csv', 'w', newline='') as file:\n",
    "                #    writer = csv.writer(file)\n",
    "                #    writer.writerow([model_name, epoch, epoch_acc_val, epoch_loss_val])\n",
    "\n",
    "                print(\"Validation Epoch {}/{}, Loss: {:.3f}, Accuracy: \\033[1m {:.3f} \\033[0m\".format(epoch+1,epochs, epoch_loss_val, epoch_acc_val))\n",
    "                #tensorboard_logging(logger, epoch_loss_val, epoch_acc_val, epoch, model)\n",
    "                val_acc_history.append(epoch_acc_val)\n",
    "                val_loss_history.append(epoch_loss_val)\n",
    "                experiment.log_metric('epoch_acc_val', epoch_acc_val*100)\n",
    "                experiment.log_metric('epoch_loss_val', epoch_loss_val)\n",
    "                \n",
    "                #deep copy the model\n",
    "                if epoch_acc_val > best_acc_val:\n",
    "                    best_acc_val = epoch_acc_val\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    # save/load best model weights\n",
    "                    if savename is not None:\n",
    "                        torch.save(model, savename+'_'+model_name)\n",
    "\n",
    "        time_elapsed = time.time() - since_epoch\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    time_elapsed = time.time() - since_total\n",
    "    print('All epochs comlete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    #with open('save_model_timing.csv', 'w', newline='') as file:\n",
    "        #writer = csv.writer(file)\n",
    "        #writer.writerow([model_name, time_elapsed])\n",
    "\n",
    "    return model, train_acc_history, val_acc_history, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #file = 'batch_size'\n",
    "    #writer = csv.writer(file)\n",
    "    for batch_size in params['batch_size']:\n",
    "        \n",
    "        print('NEW BATCH SIZE: ', batch_size) \n",
    "        train_loader, val_loader = load_split_train_val(\n",
    "            class_names=params['class_names'], \n",
    "            datadir=params['data_dir'],\n",
    "            batch_size=batch_size,\n",
    "            show_sample=False,\n",
    "            num_workers=num_workers)\n",
    "\n",
    "        dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "        model_train_accs = []\n",
    "        model_val_accs = []  \n",
    "        model_train_loss = []\n",
    "        model_val_loss = []\n",
    "        for model_name in params['model_names']: \n",
    "            for epochs in params['max_epochs']:\n",
    "                model_ft, train_acc_history, val_acc_history, train_loss_history, val_loss_history= train_model(\n",
    "                    model_name,\n",
    "                    params['savename'],\n",
    "                    dataloaders_dict,\n",
    "                    epochs, \n",
    "                    num_classes,\n",
    "                    experiment,\n",
    "                    is_inception=False\n",
    "                )\n",
    "                \n",
    "                model_val_accs.append(val_acc_history)\n",
    "                model_train_accs.append(train_acc_history)\n",
    "                model_train_loss.append(train_loss_history)\n",
    "                model_val_loss.append(val_loss_history)\n",
    "                \n",
    "                \n",
    "    return model_name, model_train_accs, model_val_accs, model_train_loss, model_val_loss, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://www.comet.ml/vprzybylo/multi-campaigns/ee93461103f3482892b60d1167e9e06c\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe2b4d878d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BATCH SIZE:  128\n",
      "counts per class:  [2393, 2073, 3050, 778, 767, 1850, 1572, 1676, 481, 612, 1896, 1342, 1226]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f0e630bc7f428db65fed98f6fecb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU(inplace=True)\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (45): ReLU(inplace=True)\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): ReLU(inplace=True)\n",
      "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (51): ReLU(inplace=True)\n",
      "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.0, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=13, bias=True)\n",
      "  )\n",
      ")\n",
      "True\n",
      "Using 2 GPUs!\n",
      "Params to learn:\n",
      "<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2b48cfd50>\n",
      "--------------------\n",
      "Phase: train\n",
      "Training, Batch 5/155, Loss: 55.634, Accuracy: 0.102\n",
      "Training, Batch 10/155, Loss: 24.454, Accuracy: 0.078\n",
      "Training, Batch 15/155, Loss: 10.655, Accuracy: 0.156\n",
      "Training, Batch 20/155, Loss: 3.398, Accuracy: 0.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a87ffbc6c18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_train_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_val_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ba67f3c6abf5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a52ae3864d50>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, savename, dataloaders_dict, epochs, num_classes, experiment, is_inception, feature_extract)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mlabel_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d5cb12b619e7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# this is what ImageFolder normally returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moriginal_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImageFolderWithPaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# the image file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 )\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "        \n",
    "    params = {'lr': [0.01],\n",
    "        'batch_size': [128, 256, 512, 1024],\n",
    "        'max_epochs': [20],\n",
    "        'data_dir':'../cpi_data/training_datasets/hand_labeled_resized_multcampaigns_clean/',\n",
    "        'optimizer':[torch.optim.Adam, torch.optim.Adagrad, torch.optim.Adadelta, torch.optim.Adamax],\n",
    "        #'momentum': [0.9, 0.999], \n",
    "        'class_names':['aggregates','blank','blurry','budding','bullets','columns','compact irregulars',\\\n",
    "                       'fragments','needles','plates','rimed aggregates','rimed columns','spheres'],\n",
    "        'model_names':['vgg19'],\n",
    "        #'model_names':['resnet18', 'resnet34', 'resnet152', 'alexnet', 'vgg16', 'vgg19', 'densenet169', 'densenet201'],\n",
    "        #'savename': '../saved_models/bs128_e50_13classes_clean'}\n",
    "        'savename': None}\n",
    "\n",
    "    experiment.log_parameters(params)\n",
    "    experiment.log_code = True\n",
    "    #experiment.add_tag('inlcudes bad data')\n",
    "    #experiment.add_tag(' labels')\n",
    "    num_workers = 0  #change to # of cores available to load images\n",
    "    num_classes = len(params['class_names'])\n",
    "    experiment.display()\n",
    "    model_name, model_train_accs, model_val_accs, model_train_loss, model_val_loss, train_loader, val_loader = main()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY PLOT for training and validation\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i.cpu().numpy()*100 for i in model_train_accs[0]], label='train')\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i.cpu().numpy()*100 for i in model_val_accs[0]], label='validation')\n",
    " \n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params['max_epochs'][0]+1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "\n",
    "#LOSS PLOT for training and validation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i for i in model_train_loss[0]], label='train')\n",
    "plt.plot(np.arange(1,(params['max_epochs'][0]+1)),[i for i in model_val_loss[0]], label='validation')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(1, (params['max_epochs'][0]+1), 10.0))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Confusion Matrix - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../saved_models/vgg19_bs128_e20_13classes_clean').cuda()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Send the model to GPU\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "all_preds= []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, (imgs, labels, img_paths) in enumerate(val_loader):\n",
    "        # get the inputs\n",
    "        inputs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "        pred = torch.argmax(output, 1)\n",
    "\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZED\n",
    "\n",
    "cm = confusion_matrix(np.asarray(list(itertools.chain(*all_preds))), np.asarray(list(itertools.chain(*all_labels))))\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(13,9))\n",
    "\n",
    "heat = sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=params['class_names'], yticklabels=params['class_names'], cmap=\"Blues\")\n",
    "heat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=18)\n",
    "heat.set_yticklabels(heat.get_xticklabels(), rotation=0, fontsize=18)\n",
    "\n",
    "\n",
    "plt.ylabel('Actual Labels', fontsize=20)\n",
    "plt.xlabel('Predicted Labels', fontsize=20);\n",
    "plt.savefig('../plots/norm_conf_matrix.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.asarray(list(itertools.chain(*all_preds))), np.asarray(list(itertools.chain(*all_labels))))\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "heat = sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=params['class_names'], yticklabels=params['class_names'], cmap=\"Blues\")\n",
    "heat.set_xticklabels(heat.get_xticklabels(), rotation=90)\n",
    "plt.ylabel('Actual Labels', fontsize=20)\n",
    "plt.xlabel('Predicted Labels', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics classification report\n",
    "classification_report(all_labels, all_preds, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning method\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 50\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue','darkblue','gold','red', 'darkred', 'lightgreen', 'darkgreen']\n",
    "for i, (model, train_accs, val_accs) in enumerate(zip(params['model_names'], model_train_accs, model_val_accs)):\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in train_accs[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in val_accs[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "plt.ylim(20,100)\n",
    "plt.xlim(1,num_epochs)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for i, (model,train_loss, val_loss) in enumerate(zip(params['model_names'], model_train_loss, model_val_loss)):\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in train_loss[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in val_loss[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.ylim(0,2.4)\n",
    "plt.xlim(1,num_epochs)\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2))\n",
    "plt.tight_layout()\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "# fig.savefig('cpi_data/OLYMPEX/plots/loss_acc_allmodels_reducelr_all_512_0dp.eps')\n",
    "# fig.savefig('cpi_data/OLYMPEX/plots/loss_acc_allmodels_reducelr_all_512_0dp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning method\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 50\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue','darkblue','gold','red', 'darkred', 'lightgreen', 'darkgreen']\n",
    "for i, (model, train_accs, val_accs) in enumerate(zip(params['model_names'], model_train_accs, model_val_accs)):\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in train_accs[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax1.scatter(np.arange(1,(num_epochs+1)), [i.cpu().numpy()*100 for i in val_accs[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "plt.ylim(40,100)\n",
    "plt.xlim(1,num_epochs)\n",
    "ax1.legend(title='Model type:', loc='lower right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for i, (model,train_loss, val_loss) in enumerate(zip(params['model_names'], model_train_loss, model_val_loss)):\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in train_loss[:num_epochs]], c=colors[i], marker='*')\n",
    "    ax2.scatter(np.arange(1,(num_epochs+1)), [i for i in val_loss[:num_epochs]], c=colors[i], marker='o', label=str(model))\n",
    "ax2.legend(title='Model type:', loc='upper right', prop={'size': 10})\n",
    "plt.ylim(0,2.4)\n",
    "plt.xlim(1,num_epochs)\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2))\n",
    "plt.tight_layout()\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "#fig.savefig('../plots/loss_acc_allmodels_bs_128_e20_13classes.eps', dpi=300)\n",
    "#fig.savefig('../plots/loss_acc_allmodels_bs_128_e20_13classes.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,5))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "num_epochs = 20\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "colors = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "color_key = {'vgg19': 'darkred', 'vgg16': 'red', 'resnet34':'salmon', 'resnet18':'lightsalmon', 'resnet152':'bisque',\n",
    "            'densenet201':'lightgreen', 'densenet169':'darkgreen', 'alexnet':'lightblue'}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = [i.cpu().numpy()*100 for i in np.array(model_train_accs)[:,epoch]]\n",
    "    model_names, train_acc, colors_sorted = (list(x) for x in zip(*sorted(zip(params['model_names'], train_acc, colors), reverse=True)))\n",
    "    colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax1.bar(epoch+1, train_acc[m], color=colors_sorted[m], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, train_acc, color=colors_sorted)\n",
    "        \n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = [i for i in np.array(model_train_loss)[:,epoch]]\n",
    "    train_loss, model_names = (list(x) for x in zip(*sorted(zip(train_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#plt.ylim(0,2)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "\n",
    "#colors = plt.cm.rainbow(np.linspace(0,1,9))\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    model_names, val_acc, colors_sorted = (list(x) for x in zip(*sorted(zip(params['model_names'], val_acc, colors), reverse=True)))\n",
    "\n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name])\n",
    "        \n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax3.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax3.yaxis.set_ticks_position('both')\n",
    "ax3.minorticks_on()\n",
    "ax3.tick_params(axis='y', which='minor', direction='out')\n",
    "ax3.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_loss, params['model_names'], colors), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "plt.ylim(0,2)\n",
    "ax4.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax4.yaxis.set_ticks_position('both')\n",
    "ax4.minorticks_on()\n",
    "ax4.tick_params(axis='y', which='minor', direction='out')\n",
    "ax4.xaxis.set_tick_params(which='minor', bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(12,5))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "num_epochs = 20\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "colors = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']  \n",
    "color_key = {'vgg19': 'darkred', 'vgg16': 'red', 'resnet34':'salmon', 'resnet18':'lightsalmon', 'resnet152':'bisque',\n",
    "            'densenet201':'lightgreen', 'densenet169':'darkgreen', 'alexnet':'lightblue'}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = [i.cpu().numpy()*100 for i in np.array(model_train_accs)[:,epoch]]\n",
    "    train_acc, model_names = (list(x) for x in zip(*sorted(zip(train_acc, params['model_names']), reverse=True)))\n",
    "    #colors_sorted = ['darkred', 'red', 'salmon', 'lightsalmon','bisque', 'lightgreen', 'darkgreen', 'lightblue']     \n",
    "            \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            print(color_key[model_name], train_acc[m], model_name)\n",
    "            p1 = ax1.bar(epoch+1, train_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, train_acc, color=colors_sorted)\n",
    "        \n",
    "plt.ylim(30,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = [i for i in np.array(model_train_loss)[:,epoch]]\n",
    "    train_loss, model_names = (list(x) for x in zip(*sorted(zip(train_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, train_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#plt.ylim(0,2)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "colors_sorted = ['darkred', 'red', 'salmon', \\\n",
    "                 'lightsalmon','bisque', 'lightgreen', \\\n",
    "                 'darkgreen', 'lightblue']  \n",
    "\n",
    "#colors = plt.cm.rainbow(np.linspace(0,1,9))\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    val_acc, model_names = (list(x) for x in zip(*sorted(zip(val_acc, params['model_names']), reverse=True)))\n",
    "\n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            print(color_key[model_name], val_acc[m], model_name)\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name], label=model_name)\n",
    "            \n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax3.bar(epoch+1, val_acc[m], color=color_key[model_name])\n",
    "        \n",
    "plt.ylim(30,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax3.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax3.yaxis.set_ticks_position('both')\n",
    "ax3.minorticks_on()\n",
    "ax3.tick_params(axis='y', which='minor', direction='out')\n",
    "ax3.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names= (list(x) for x in zip(*sorted(zip(val_loss, params['model_names']), reverse=True)))\n",
    "    \n",
    "    if epoch == params['max_epochs'][0]-1: \n",
    "    #if epoch == 0:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name], label=model_name)\n",
    "    else:\n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax4.bar(epoch+1, val_loss[m], color=color_key[model_name])\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "plt.ylim(0,2)\n",
    "ax4.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax4.yaxis.set_ticks_position('both')\n",
    "ax4.minorticks_on()\n",
    "ax4.tick_params(axis='y', which='minor', direction='out')\n",
    "ax4.xaxis.set_tick_params(which='minor', bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = np.array(model_train_accs).transpose(1,0)*100\n",
    "df_train = pd.DataFrame(train_accs, index=epochs, columns=params['model_names'], dtype = np.float64)\n",
    "val_accs = np.array(model_val_accs).transpose(1,0)*100\n",
    "df_val = pd.DataFrame(val_accs, index=epochs, columns=params['model_names'], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(kind='bar', colormap='rainbow', stacked=False, figsize=(12,5), ylim=[20,120], xlim=[0,29]).legend(\n",
    "    loc='upper center', ncol=4, title=\"Model Name\")\n",
    "plt.axhline(y=100, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "\n",
    "df_val.plot(kind='bar', colormap='rainbow', stacked=False, figsize=(12,5), ylim=[20,120], xlim=[0,29]).legend(\n",
    "    loc='upper center', ncol=4, title=\"Model Name\")\n",
    "plt.axhline(y=100, color='k', linestyle='--', lw=1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "num_epochs = 10\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "colors = ['lightblue', 'blue', 'darkblue', 'gold','red', 'darkred', 'lightgreen', 'darkgreen']  \n",
    "width = 0.35\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = [i.cpu().numpy()*100 for i in np.array(model_val_accs)[:,epoch]]\n",
    "    val_acc, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_acc, params['model_names'], colors), reverse=True)))\n",
    "\n",
    "    if epoch == 0: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax1.bar(epoch+1, val_acc[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "            p1 = ax1.bar(epoch+1+width, val_acc[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "    else:\n",
    "        p1 = ax1.bar(epoch+1, val_acc, color=colors_sorted)\n",
    "\n",
    "plt.ylim(20,100)\n",
    "plt.xlim(0,num_epochs+1)\n",
    "#ax1.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='y', which='minor', direction='out')\n",
    "ax1.xaxis.set_tick_params(which='minor', bottom=False)\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = [i for i in np.array(model_val_loss)[:,epoch]]\n",
    "    val_loss, model_names, colors_sorted = (list(x) for x in zip(*sorted(zip(val_loss, params['model_names'], colors), reverse=True)))\n",
    "\n",
    "    if epoch == 0: \n",
    "        for m, model_name in enumerate(model_names):\n",
    "            p1 = ax2.bar(epoch+1, val_loss[m], width=width, color=colors_sorted[m], label=model_name)\n",
    "    else:\n",
    "        p1 = ax2.bar(epoch+1, val_loss, color=colors_sorted)\n",
    "\n",
    "\n",
    "plt.xlim(0,num_epochs+1)\n",
    "ax2.legend(title='Model type:', loc='right', prop={'size': 10})\n",
    "plt.xticks(np.arange(1, num_epochs+1, 2.0))\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "ax2.minorticks_on()\n",
    "ax2.tick_params(axis='y', which='minor', direction='out')\n",
    "ax2.xaxis.set_tick_params(which='minor', bottom=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Predictions on Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns a Numpy array\n",
    "    '''\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(path, model, topk=9):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    img = Image.open(path)\n",
    "    img = img.convert('RGB')\n",
    "    img = process_image(img)\n",
    "    \n",
    "    # Convert 2D image to 1D vector\n",
    "    img = np.expand_dims(img, 0)\n",
    "\n",
    "    img = torch.from_numpy(img)\n",
    "    \n",
    "    model.eval()\n",
    "    inputs = Variable(img).to(device)\n",
    "    logits = model.forward(inputs)\n",
    "    \n",
    "    ps = F.softmax(logits,dim=1)\n",
    "    topk = ps.cpu().topk(topk)\n",
    "    \n",
    "    return (e.data.numpy().squeeze().tolist() for e in topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(im, prob, crystal_names):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    \n",
    "    image = Image.open(im)\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(7, 10), ncols=1, nrows=2)\n",
    "    \n",
    "    ax1.set_title(crystal_names[0])\n",
    "    ax1.imshow(image)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    y_pos = np.arange(len(prob))\n",
    "    ax2.barh(y_pos, prob, align='center')\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(crystal_names)\n",
    "    ax2.tick_params(axis='y', rotation=45)\n",
    "    ax2.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax2.set_title('Class Probability')\n",
    "    plt.show()\n",
    "    #current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #fig.savefig('classify/'+current_time+'.png',bbox_inches='tight',pad_inches=.3)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load('../saved_models/vgg19_bs128_e20_13classes').cuda()\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_loader = torch.load('../saved_models/val_loader.pth')\n",
    "for batch_idx, (imgs, labels, img_paths) in enumerate(val_loader):\n",
    "    #predictions = model_ft(imgs)\n",
    "    #preds = torch.max(predictions, 1).indices.tolist()    \n",
    "    \n",
    "    for im in img_paths:\n",
    "        probs, classes = predict2(im, model.to(device))  \n",
    "        crystal_names = [params['class_names'][e] for e in classes]\n",
    "        view_classify(im, probs, crystal_names)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(data_dir, save_dir, crystal_names, im):\n",
    "    image = Image.open(data_dir + im).convert(\"RGB\")\n",
    "    if crystal_names[0] == 'rimed aggs':\n",
    "        crystal_names[0] = 'rimed_aggs'\n",
    "    if crystal_names[0] == 'rimed columns':\n",
    "        crystal_names[0] = 'rimed_columns'\n",
    "    if crystal_names[0] == 'compact irregulars':\n",
    "         crystal_names[0] = 'compact_irregulars'\n",
    "    if not os.path.exists(save_dir+crystal_names[0]):\n",
    "        os.makedirs(save_dir+crystal_names[0])\n",
    "    image.save(save_dir+crystal_names[0]+'/'+im) \n",
    "    # cv2.imwrite(save_dir+crystal_names[0]+'/'+im, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on new data - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc)\n",
    "        #print(image)\n",
    "        #image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        tensor_image = self.transform(image)\n",
    "        path = self.total_imgs[idx]\n",
    "        return tensor_image, path\n",
    "\n",
    "model = torch.load('../saved_models/bs128_e50_13classes_clean_vgg19').cuda()\n",
    "model.eval()\n",
    "campaign = 'ARM'\n",
    "data_dir = '../cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "#save_dir = 'cpi_data/campaigns/'+campaign+'/'\n",
    "\n",
    "#apply same transforms\n",
    "test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testdata = TestDataSet(data_dir, transform=test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=100, shuffle=False, \n",
    "                               num_workers=20, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, open_dir, file_list):\n",
    "        self.desired_size = 1000\n",
    "        self.open_dir = open_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        self.all_paths = natsorted(file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.open_dir, self.all_paths[idx])\n",
    "        #image = Image.open(img_path)\n",
    "        \n",
    "        #training images were resized to 1000x1000 initially\n",
    "        image = cv2.cvtColor(cv2.imread(self.open_dir+self.all_paths[idx], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.desired_size, self.desired_size), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        image = Image.fromarray(image) #convert back to PIL for transforms\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        path = self.all_paths[idx]\n",
    "        return (image, path)\n",
    "    \n",
    "model = torch.load('../saved_models/bs128_e50_13classes_clean_vgg19').cuda()\n",
    "model.eval()\n",
    "campaign = 'MPACE'\n",
    "df = pd.read_pickle('../final_databases/no_mask/df_good_ice_'+campaign+'.pkl')\n",
    "data_dir = '../cpi_data/campaigns/'+campaign+'/single_imgs/'\n",
    "testdata = TestDataSet(data_dir, df['filename'])\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=100, shuffle=False, \n",
    "                               num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for batch_idx, (imgs, img_paths) in enumerate(test_loader):\n",
    "    #predictions = model_ft(imgs)\n",
    "    #preds = torch.max(predictions, 1).indices.tolist()   \n",
    "    for im in img_paths:\n",
    "        path = data_dir + im\n",
    "        probs, classes = predict2(path, model.to(device))\n",
    "        crystal_names = [params['class_names'][e] for e in classes]\n",
    "        view_classify(path, probs, crystal_names)\n",
    "        #save_image(data_dir, save_dir, crystal_names, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_idx, (imgs, img_paths) in enumerate(test_loader):\n",
    "    for im in img_paths:\n",
    "        path = data_dir+im\n",
    "        img_og = Image.open(path)\n",
    "        img = img_og.convert('RGB')\n",
    "        img = process_image(img)\n",
    "\n",
    "        # Convert 2D image to 1D vector\n",
    "        img = np.expand_dims(img, 0)\n",
    "\n",
    "        img = torch.from_numpy(img)\n",
    "        prediction = model(img)\n",
    "        cpu_pred = prediction.cpu()\n",
    "        result = cpu_pred.data.numpy()\n",
    "        print(class_names[result.argmax()])\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.imshow(img_og)\n",
    "        plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
