{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bdf0ebe427dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#from comet_ml import Experiment\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numba\n",
    "import umap \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test(num_workers, datadir, valid_size = .2):\n",
    "    train_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                        transforms.ToTensor(),                                   \n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir,\n",
    "                    transform=test_transforms)\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    print('number of workers to load data', num_workers)\n",
    "    print('batch size: ',hyper_params['batch_size'])\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=hyper_params['batch_size'], num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=hyper_params['batch_size'], num_workers=num_workers)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cpu(trainloader, testloader):\n",
    "    train_imgs, train_label = next(iter(trainloader))\n",
    "    test_imgs, test_label = next(iter(testloader))\n",
    "\n",
    "    # Convert image to numpy\n",
    "    train_imgs_np = train_imgs.to('cpu').numpy()\n",
    "    test_imgs_np = test_imgs.to('cpu').numpy()\n",
    "\n",
    "    train_labs_np = train_label.to('cpu').numpy()\n",
    "    test_labs_np = test_label.to('cpu').numpy()\n",
    "    print(np.shape(train_imgs_np))\n",
    "    \n",
    "    nsamples1, nz, nx, ny = np.shape(train_imgs_np)\n",
    "    nsamples2, nz, nx, ny = np.shape(test_imgs_np)\n",
    "    print(nz, nx, ny)\n",
    "\n",
    "    train_imgs_np = train_imgs_np.reshape((nsamples1,nz*nx*ny))\n",
    "    test_imgs_np = test_imgs_np.reshape((nsamples2,nz*nx*ny))\n",
    "\n",
    "    data = np.array(np.vstack([train_imgs_np, test_imgs_np]), dtype=np.float64)\n",
    "\n",
    "    return (train_imgs_np, train_labs_np, test_imgs_np, test_labs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(train_imgs_np, train_labs_np, test_imgs_np, test_labs_np):        \n",
    "    \n",
    "    #can only write 4gb at a time so had to split into 4 chunks\n",
    "    with open('train_imgs','wb') as f:\n",
    "        pickle.dump(train_imgs_np, f, protocol=4)\n",
    "    with open('train_labs','wb') as f:\n",
    "        pickle.dump(train_labs_np, f, protocol=4)\n",
    "    with open('test_imgs','wb') as f:\n",
    "        pickle.dump(test_imgs_np, f, protocol=4)\n",
    "    with open('test_labs','wb') as f:\n",
    "        pickle.dump(test_labs_np, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_imgs_np = np.array(np.load('train_imgs'))\n",
    "    train_labs_np = np.array(np.load('train_labs'))\n",
    "    test_imgs_np = np.array(np.load('test_imgs'))\n",
    "    test_labs_np = np.array(np.load('test_labs'))\n",
    "    \n",
    "    print('done loading arrays')\n",
    "    \n",
    "    return (train_imgs_np, train_labs_np, test_imgs_np, test_labs_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_imgs_np = np.array(np.load('cpi_data/train_imgs'))\n",
    "    train_labs_np = np.array(np.load('cpi_data/train_labs'))\n",
    "    test_imgs_np = np.array(np.load('cpi_data/test_imgs'))\n",
    "    test_labs_np = np.array(np.load('cpi_data/test_labs'))\n",
    "    \n",
    "    print('done loading arrays')\n",
    "    \n",
    "    return (train_imgs_np, train_labs_np, test_imgs_np, test_labs_np)\n",
    "\n",
    "train_imgs_np, train_labs_np, test_imgs_np, test_labs_np = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 10\n",
    "data_dir = 'cpi_data/'\n",
    "valid_size = 0.2\n",
    "hyper_params = {\n",
    "    \"num_classes\": 3,\n",
    "    \"input_size\": 2048,\n",
    "    \"hidden_size\": 512,   \n",
    "    \"batch_size\": 11100,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.0002   #The lower the value, the slower we travel along the downward slope\n",
    "}\n",
    "classes=['aggs','junk','columns']\n",
    "\n",
    "trainloader, testloader = load_split_train_test(num_workers=num_workers, datadir=data_dir, valid_size=valid_size)\n",
    "train_imgs_np, train_labs_np, test_imgs_np, test_labs_np = convert_to_cpu(trainloader, testloader)\n",
    "#print('Done converting to np on CPU')\n",
    "write_data(train_imgs_np, train_labs_np, test_imgs_np, test_labs_np)\n",
    "#print('Done writing')\n",
    "#train_imgs_np, train_labs_np, test_imgs_np, test_labs_np = load_data()\n",
    "\n",
    "#%time embedding = umap.UMAP(n_neighbors=50).fit_transform(train_imgs_np)\n",
    "#print('Done with UMAP')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz, nx, ny = 3, 224, 224\n",
    "train_imgs = train_imgs_np.reshape((8887,nx,ny,nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_imgs_np))\n",
    "class_names = ['aggs', 'columns','junk']\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_imgs[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labs_np[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time trans = umap.UMAP(n_neighbors=50).fit(train_imgs_np, train_labs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('trans', 'rb')\n",
    "trans = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(trans))\n",
    "%time trans_train=trans.transform(train_imgs_np) \n",
    "%time trans_test=trans.transform(test_imgs_np) \n",
    "print(type(trans_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'trans_test_50'\n",
    "#filehandler = open(filename, 'wb')\n",
    "#%time pickle.dump(trans_test, filehandler, protocol=4)\n",
    "#filehandler.close()\n",
    "\n",
    "#with open('trans_test_50.msgpack', 'wb') as outfile:\n",
    "#    embedding = msgpack.pack(trans_test, outfile)\n",
    "#with open('trans.msgpack', 'rb') as data_file:\n",
    "#    embedding = msgpack.unpack(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_imgs_np), np.shape(test_imgs_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time trans = umap.UMAP(n_neighbors=5, random_state=42).fit(train_imgs_np, )\n",
    "#embedded into two dimensions in the locations by class\n",
    "plt.scatter(trans.embedding_[:, 0], trans.embedding_[:, 1], s= 5, c=train_labs_np, cmap='Spectral')\n",
    "plt.title('Embedding of the training set by UMAP', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time embedding = umap.UMAP(n_neighbors=15).fit_transform(train_imgs_np)\n",
    "#We can now train some new models (again an SVC and a KNN classifier) on the embedded training data\n",
    "svc = SVC().fit(trans.embedding_, train_labs_np)\n",
    "knn = KNeighborsClassifier().fit(trans.embedding_, train_labs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(trans.transform(test_imgs_np), test_labs_np), knn.score(trans.transform(test_imgs_np), test_labs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform() method on that model to transform the test set into the learned space\n",
    "%time test_embedding = trans.transform(test_imgs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_embedding[:, 0], test_embedding[:, 1], s= 5, c=test_labs_np, cmap='Spectral')\n",
    "plt.title('Embedding of the test set by UMAP', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*embedding.T, s=50, c=target, alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(4))\n",
    "cbar.set_ticks(np.arange(3))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.title('Habit Classification via UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*embedding.T, s=50, c=target, alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(4))\n",
    "cbar.set_ticks(np.arange(3))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.title('Habit Classification via UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*embedding.T, s=50, c=target, alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(4))\n",
    "cbar.set_ticks(np.arange(3))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.title('Habit Classification via UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*embedding.T, s=50, c=target, alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(4))\n",
    "cbar.set_ticks(np.arange(3))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.title('Habit Classification via UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*embedding.T, s=50, c=target, alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(4))\n",
    "cbar.set_ticks(np.arange(3))\n",
    "cbar.set_ticklabels(classes)\n",
    "plt.title('Habit Classification via UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_umap(n_neighbors=150, min_dist=0.1, n_components=3, metric='euclidean', title=''):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    u = fit.fit_transform(data);\n",
    "    fig = plt.figure()\n",
    "    if n_components == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], range(len(u)), c=target)\n",
    "    if n_components == 2:\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(u[:,0], u[:,1], c=target)\n",
    "    if n_components == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(u[:,0], u[:,1], u[:,2], c=target, s=100)\n",
    "    plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.hstack([train_labels_np, test_labels_np])\n",
    "for n in (5, 25, 50, 100, 200):\n",
    "    draw_umap(n_neighbors=n, title='n_neighbors = {}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = umap.UMAP(n_neighbors=15).fit_transform(data)\n",
    "%time svc = SVC().fit(train_imgs_np, train_labels_np)\n",
    "%time knn = KNeighborsClassifier().fit(train_imgs_np, train_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time svc.score(test_imgs_np, test_labels_np), knn.score(test_imgs_np, test_labels_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
